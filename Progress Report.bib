
@misc{ho_imagen_2022,
	title = {Imagen Video: High Definition Video Generation with Diffusion Models},
	url = {http://arxiv.org/abs/2210.02303},
	doi = {10.48550/arXiv.2210.02303},
	shorttitle = {Imagen Video},
	abstract = {We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial super-resolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See https://imagen.research.google/video/ for samples.},
	number = {{arXiv}:2210.02303},
	publisher = {{arXiv}},
	author = {Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P. and Poole, Ben and Norouzi, Mohammad and Fleet, David J. and Salimans, Tim},
	urldate = {2023-04-05},
	date = {2022-10-05},
	eprinttype = {arxiv},
	eprint = {2210.02303 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, {EduViz}},
	annotation = {Comment: See accompanying website: https://imagen.research.google/video/},
	file = {arXiv Fulltext PDF:/Users/gustavoramirez/Zotero/storage/KYH726SS/Ho et al. - 2022 - Imagen Video High Definition Video Generation wit.pdf:application/pdf;arXiv.org Snapshot:/Users/gustavoramirez/Zotero/storage/DIHICADL/2210.html:text/html},
}

@inproceedings{sarkar_personalized_2021,
	title = {Personalized Learning Path Generation in E-Learning Systems using Reinforcement Learning and Generative Adversarial Networks},
	doi = {10.1109/SMC52423.2021.9658967},
	abstract = {Accelerated by the pandemic and the resulting increased adoption of online classes by universities, E-learning and the challenge to increase its efficiency in terms of conveying content effectively to the individual learner has gained interest and importance. To achieve this customization, personalization of the learning path and the learning objects based on the characteristics of the learner could form an important component that is currently largely absent from the most used e-learning modalities. This paper introduces an approach to personalization of the learning path that attempts to optimize the learner’s performance using Reinforcement Learning based on implicit feedback while minimizing the need for actual interaction with the learning system during training. The proposed system adopts the concepts of the Felder and Silverman Learning Style Model and Differentiated Pedagogy and builds an architecture using two interacting machine learning components, one using Reinforcement Learning to optimize the learning path and learning objects, and one using Conditional Generative Adversarial Networks to rapidly adapt a model of the learner’s characteristics. The model strives to minimize student interactions to learn about the learner’s performance characteristics and to generate personalized learning paths, thus reducing negative experiences during training. For this, a Conditional Generative Adversarial Networks is used that can rapidly adapt to provide realistic simulations of the student’s performance to use in the training of the e-learning strategy and thus to help reduce the need for actual learner feedback. In a set of base experiments with synthetic data, the model is shown to be effective at learning personalization for different learner types and efficient compared to models without Generative Adversarial Networks.},
	eventtitle = {2021 {IEEE} International Conference on Systems, Man, and Cybernetics ({SMC})},
	pages = {92--99},
	booktitle = {2021 {IEEE} International Conference on Systems, Man, and Cybernetics ({SMC})},
	author = {Sarkar, Subharag and Huber, Manfred},
	date = {2021-10},
	note = {{ISSN}: 2577-1655},
	keywords = {Adaptation models, Adaptive systems, Conferences, {EduViz}, Electronic learning, Pandemics, Reinforcement learning, Training},
	file = {IEEE Xplore Abstract Record:/Users/gustavoramirez/Zotero/storage/S2PE82SN/9658967.html:text/html;IEEE Xplore Full Text PDF:/Users/gustavoramirez/Zotero/storage/ICHRAGXN/Sarkar and Huber - 2021 - Personalized Learning Path Generation in E-Learnin.pdf:application/pdf},
}

@article{diwan_ai-based_2023,
	title = {{AI}-based learning content generation and learning pathway augmentation to increase learner engagement},
	volume = {4},
	issn = {2666-920X},
	url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000650},
	doi = {10.1016/j.caeai.2022.100110},
	abstract = {Retaining learner engagement is a major challenge in online learning environments, which is even more intensified with learning spaces increasingly built by combining resources from multiple independent sources. Narrative-centric learning experience has been found to improve learner engagement by several researchers. Towards this end, we propose an {AI}-based approach that generates auxiliary learning content called narrative fragments which are interspersed into the learning pathways to create interactive learning narratives. The proposed approach consists of the automatic generation of two types of narrative fragments– overviews of the learning pathway segments and reflection quizzes or formative assessments from learning resources in any format including open educational resources. The pipeline for the generation of the narrative fragments consists of various components based on different semantic models and a natural language generation ({NLG}) component based on a pre-trained language model {GPT}-2 (Generative Pre-trained Transformer 2). Automation enables the generation of narrative fragments on the fly whenever there are changes in the learning pathway due to the need for reiteration of concepts, pre-requisite knowledge acquisition, etc., enabling adaptability in the learning pathways. The proposed approach is domain agnostic which makes it easily adaptable to different domains. The {NLG} model is evaluated using {ROUGE} scores against several baselines. Automatically generated narrative fragments are evaluated by human evaluators. We obtained encouraging results in both cases.},
	pages = {100110},
	journaltitle = {Computers and Education: Artificial Intelligence},
	shortjournal = {Computers and Education: Artificial Intelligence},
	author = {Diwan, Chaitali and Srinivasa, Srinath and Suri, Gandharv and Agarwal, Saksham and Ram, Prasad},
	urldate = {2023-04-11},
	date = {2023-01-01},
	langid = {english},
	keywords = {Automatic question generation, Curating learning pathways, Definition generation, Educational content generation, {EduViz}, Language models, Learner engagement, Multiple choice question, Open educational resources},
	file = {ScienceDirect Full Text PDF:/Users/gustavoramirez/Zotero/storage/UVVWW5UW/Diwan et al. - 2023 - AI-based learning content generation and learning .pdf:application/pdf;ScienceDirect Snapshot:/Users/gustavoramirez/Zotero/storage/429VQBJR/S2666920X22000650.html:text/html},
}

@misc{shen_hugginggpt_2023,
	title = {{HuggingGPT}: Solving {AI} Tasks with {ChatGPT} and its Friends in {HuggingFace}},
	url = {http://arxiv.org/abs/2303.17580},
	doi = {10.48550/arXiv.2303.17580},
	shorttitle = {{HuggingGPT}},
	abstract = {Solving complicated {AI} tasks with different domains and modalities is a key step toward advanced artificial intelligence. While there are abundant {AI} models available for different domains and modalities, they cannot handle complicated {AI} tasks. Considering large language models ({LLMs}) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that {LLMs} could act as a controller to manage existing {AI} models to solve complicated {AI} tasks and language could be a generic interface to empower this. Based on this philosophy, we present {HuggingGPT}, a framework that leverages {LLMs} (e.g., {ChatGPT}) to connect various {AI} models in machine learning communities (e.g., Hugging Face) to solve {AI} tasks. Specifically, we use {ChatGPT} to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected {AI} model, and summarize the response according to the execution results. By leveraging the strong language capability of {ChatGPT} and abundant {AI} models in Hugging Face, {HuggingGPT} is able to cover numerous sophisticated {AI} tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards advanced artificial intelligence.},
	number = {{arXiv}:2303.17580},
	publisher = {{arXiv}},
	author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
	urldate = {2023-04-25},
	date = {2023-04-02},
	eprinttype = {arxiv},
	eprint = {2303.17580 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/gustavoramirez/Zotero/storage/Q5S56H89/Shen et al. - 2023 - HuggingGPT Solving AI Tasks with ChatGPT and its .pdf:application/pdf;arXiv.org Snapshot:/Users/gustavoramirez/Zotero/storage/XBTDKTGP/2303.html:text/html},
}

@misc{rombach_high-resolution_2022,
	title = {High-Resolution Image Synthesis with Latent Diffusion Models},
	url = {http://arxiv.org/abs/2112.10752},
	doi = {10.48550/arXiv.2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models ({DMs}) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful {DMs} often consumes hundreds of {GPU} days and inference is expensive due to sequential evaluations. To enable {DM} training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models ({LDMs}) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based {DMs}. Code is available at https://github.com/{CompVis}/latent-diffusion .},
	number = {{arXiv}:2112.10752},
	publisher = {{arXiv}},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	urldate = {2023-04-26},
	date = {2022-04-13},
	eprinttype = {arxiv},
	eprint = {2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annotation = {Comment: {CVPR} 2022},
	file = {arXiv Fulltext PDF:/Users/gustavoramirez/Zotero/storage/2G4ITJ7W/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf:application/pdf;arXiv.org Snapshot:/Users/gustavoramirez/Zotero/storage/FCFUGTK6/2112.html:text/html},
}
